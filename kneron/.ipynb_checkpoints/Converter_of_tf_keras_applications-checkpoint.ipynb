{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Converter of tf.keras.applications.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2nkz5rupCVs1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCfxQD0aHZO_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5f4692ab-2f58-4439-e92f-41f08461b9c1"
      },
      "source": [
        "#@title Please Choose parm, and ***Run*** this Cell\n",
        "\n",
        "parm = \"onnx\"#@param [\"tpu\", \"onnx\", \"h5\", \"all\"]\n",
        "to_tpu  = parm in [\"tpu\", \"all\"]\n",
        "to_onnx = parm in [\"onnx\", \"all\"]\n",
        "to_h5   = parm in [\"h5\", \"all\"]\n",
        "target_folder = \"target\"\n",
        "\n",
        "!rm {target_folder} -rf\n",
        "!mkdir {target_folder}\n",
        "\n",
        "print(f\"option  : {parm}\",\n",
        "      f\"to_tpu  : {to_tpu}\",\n",
        "      f\"to_onnx : {to_onnx}\",\n",
        "      f\"to_h5   : {to_h5}\",\n",
        "      f\"folder  : {target_folder}\",\n",
        "      sep='\\n')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "option  : onnx\n",
            "to_tpu  : False\n",
            "to_onnx : True\n",
            "to_h5   : False\n",
            "folder  : target\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVv05rV0YMdx",
        "colab_type": "text"
      },
      "source": [
        "#set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fITZklk_fgta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnXEV9USfn_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_names = [\n",
        "               \"Xception\",\n",
        "               \"ResNet50\",\n",
        "               \"InceptionV3\",\n",
        "              #  \"MobileNet\",\n",
        "               \"MobileNetV2\",\n",
        "              #  \"NASNetMobile\"\n",
        "]\n",
        "model_list = list(map(lambda key : vars(tf.keras.applications)[key], model_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw6nrhY4gEwj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "db441572-dc47-4cb3-cbc2-54706374be82"
      },
      "source": [
        "model_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<function tensorflow.python.keras.applications.xception.Xception>,\n",
              " <function tensorflow.python.keras.applications.resnet.ResNet50>,\n",
              " <function tensorflow.python.keras.applications.inception_v3.InceptionV3>,\n",
              " <function tensorflow.python.keras.applications.mobilenet_v2.MobileNetV2>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nkz5rupCVs1",
        "colab_type": "text"
      },
      "source": [
        "# Counting Flops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocm9AWAeCRre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "65f3c209-58dd-4cde-9b39-534fa7c9221d"
      },
      "source": [
        "def get_flops(model_h5_path):\n",
        "    session = tf.compat.v1.Session()\n",
        "    graph = tf.compat.v1.get_default_graph()\n",
        "    \n",
        "    with graph.as_default():\n",
        "        with session.as_default():\n",
        "            model = tf.keras.models.load_model(model_h5_path)\n",
        "\n",
        "            run_meta = tf.compat.v1.RunMetadata()\n",
        "            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "        \n",
        "            # We use the Keras session graph in the call to the profiler.\n",
        "            flops = tf.compat.v1.profiler.profile(graph=graph,\n",
        "                                                  run_meta=run_meta, cmd='op', options=opts)\n",
        "        \n",
        "\n",
        "            return flops.total_float_ops\n",
        "itr = 3\n",
        "for name, func in zip(model_names[itr:itr+1], model_list[itr:itr+1]):\n",
        "    now = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(name, func, now)\n",
        "    model = func()\n",
        "    target_path = \"tmp.h5\"\n",
        "    model.save(target_path)\n",
        "    print(\"*\"*100)\n",
        "\n",
        "    flops = get_flops(target_path)\n",
        "    print(name, \"\\n\\t\", f\"flops:{flops}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MobileNetV2 <function MobileNetV2 at 0x7f5373343c80> 10:02:26\n",
            "****************************************************************************************************\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "MobileNetV2 \n",
            "\t flops:7007901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS_NbAyUHTrd",
        "colab_type": "text"
      },
      "source": [
        "# tensorflow to tflite & coral tpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge3iVR0Lf4VF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "56025269-ea18-42b8-a871-e47e13bd1c74"
      },
      "source": [
        "print(f\"pram is {parm}\")\n",
        "if to_tpu:\n",
        "  ! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "  ! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "  ! sudo apt-get update\n",
        "  ! sudo apt-get install edgetpu-compiler\t\n",
        "  def randoom_representative_data_gen(model):\n",
        "    itr = 0\n",
        "    while True:\n",
        "      itr += 1\n",
        "      image = np.random.random([1,*model.layers[0]._batch_input_shape[1:]]).astype(np.float32)\n",
        "      yield [image]\n",
        "      if itr >= 100:\n",
        "        break\n",
        "  for name, func in zip(model_names, model_list):\n",
        "    now = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(name, func, now)\n",
        "    model = func()\n",
        "    \n",
        "\n",
        "    model_name = os.path.join(target_folder, f\"model_{name}.tflite\")\n",
        "    model_name_float16 = os.path.join(target_folder, f\"model_{name}_float16.tflite\")\n",
        "    model_name_quantized = os.path.join(target_folder, f\"model_{name}_quantized.tflite\")\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    with open(model_name, 'wb') as f:\n",
        "      f.write(tflite_model)\n",
        "    \n",
        "    #float16\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_types = [tf.float16]\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    with open(model_name_float16, 'wb') as f:\n",
        "      f.write(tflite_model)\n",
        "    \n",
        "    ## to tpu\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.uint8\n",
        "    converter.inference_output_type = tf.uint8\n",
        "\n",
        "    converter.representative_dataset = lambda: randoom_representative_data_gen(model)\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    with open(model_name_quantized, 'wb') as f:\n",
        "      f.write(tflite_model)\n",
        "    \n",
        "    !edgetpu_compiler {model_name_quantized}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pram is tpu\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   653  100   653    0     0  31095      0 --:--:-- --:--:-- --:--:-- 31095\n",
            "OK\n",
            "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\n",
            "Hit:1 https://packages.cloud.google.com/apt coral-edgetpu-stable InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Hit:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "edgetpu-compiler is already the newest version (14.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 68 not upgraded.\n",
            "Xception <function Xception at 0x7fafc3c246a8> 14:04:41\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ea69cc0b8164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/xception.py\u001b[0m in \u001b[0;36mXception\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         name=prefix + '_sepconv3')(x)\n\u001b[0m\u001b[1;32m    234\u001b[0m     x = layers.BatchNormalization(\n\u001b[1;32m    235\u001b[0m         axis=channel_axis, name=prefix + '_sepconv3_bn')(x)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0;31m# Check for any resource inputs. If we find any, we update control_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0;31m# and last_write_to_resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mis_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mResourceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_ONLY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m_get_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    461\u001b[0m       \u001b[0;31m# TODO(srbs): An alternate would be to just compare the old and new set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m       \u001b[0;31m# but that may not be as fast.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m       \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_acd_resource_resolvers_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;31m# Conservatively remove any resources from `reads` that are also writes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_resource_resolver\u001b[0;34m(op, resource_reads, resource_writes)\u001b[0m\n\u001b[1;32m   4540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4541\u001b[0m   \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4542\u001b[0;31m   if op.type in [\n\u001b[0m\u001b[1;32m   4543\u001b[0m       \u001b[0;34m\"DatasetToSingleElement\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetToTFRecord\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ReduceDataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4544\u001b[0m   ]:\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2219\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m     \u001b[0;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2221\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2223\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dch2064Gi44V",
        "colab_type": "text"
      },
      "source": [
        "# to onnx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SPqb0lErfvU3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e06a650-c1fe-46e3-d5c1-992936c80858"
      },
      "source": [
        "print(f\"pram is {parm}\")\n",
        "if to_onnx:\n",
        "  !pip install keras2onnx\n",
        "  import keras2onnx\n",
        "  for name, func in zip(model_names, model_list):\n",
        "    now = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(now, name, func, sep = \"\\n\")\n",
        "    model = func()\n",
        "    image_shape = model.layers.pop(0)._batch_input_shape[1:]\n",
        "    batch_is_1_shape = [1, *image_shape]\n",
        "    new_input  = tf.keras.layers.Input(batch_shape=batch_is_1_shape)\n",
        "    new_output = model(new_input)\n",
        "    new_model = tf.keras.models.Model(new_input, new_output)\n",
        "    onnx_model = keras2onnx.convert_keras(new_model, model.name)\n",
        "\n",
        "    model_path = os.path.join(target_folder,f\"{name}_{'x'.join(map(str,batch_is_1_shape))}.onnx\")\n",
        "\n",
        "    keras2onnx.save_model(onnx_model, model_path)\n",
        "    print(\"*\"*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pram is all\n",
            "Collecting keras2onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2f/c7aef8f8215c62d55ea05f5b36737c1726e4fea6c73970909523ae497fd9/keras2onnx-1.7.0-py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (3.10.0)\n",
            "Collecting onnxconverter-common>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/7a/7e30c643cd7d2ad87689188ef34ce93e657bd14da3605f87bcdbc19cd5b1/onnxconverter_common-1.7.0-py2.py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.0MB/s \n",
            "\u001b[?25hCollecting onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/ee/bc7bc88fc8449266add978627e90c363069211584b937fd867b0ccc59f09/onnx-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 10.3MB/s \n",
            "\u001b[?25hCollecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2020.6.20)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->keras2onnx) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->keras2onnx) (47.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->keras2onnx) (3.6.6)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->keras2onnx) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=a494913d9e4d910eee9a197a89c7f4f7ee83fe5f0b2abfd05c7b3b5b6da656e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "Successfully built fire\n",
            "Installing collected packages: onnx, onnxconverter-common, fire, keras2onnx\n",
            "Successfully installed fire-0.3.1 keras2onnx-1.7.0 onnx-1.7.0 onnxconverter-common-1.7.0\n",
            "09:06:59\n",
            "Xception\n",
            "<function Xception at 0x7ff40b994ae8>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "tf executing eager_mode: True\n",
            "INFO:keras2onnx:tf executing eager_mode: True\n",
            "tf.keras model eager_mode: False\n",
            "INFO:keras2onnx:tf.keras model eager_mode: False\n",
            "The ONNX operator number change on the optimization: 390 -> 132\n",
            "INFO:keras2onnx:The ONNX operator number change on the optimization: 390 -> 132\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****************************************************************************************************\n",
            "09:07:15\n",
            "ResNet50\n",
            "<function ResNet50 at 0x7ff40b986ea0>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "tf executing eager_mode: True\n",
            "INFO:keras2onnx:tf executing eager_mode: True\n",
            "tf.keras model eager_mode: False\n",
            "INFO:keras2onnx:tf.keras model eager_mode: False\n",
            "The ONNX operator number change on the optimization: 459 -> 127\n",
            "INFO:keras2onnx:The ONNX operator number change on the optimization: 459 -> 127\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****************************************************************************************************\n",
            "09:07:35\n",
            "InceptionV3\n",
            "<function InceptionV3 at 0x7ff40b977840>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "tf executing eager_mode: True\n",
            "INFO:keras2onnx:tf executing eager_mode: True\n",
            "tf.keras model eager_mode: False\n",
            "INFO:keras2onnx:tf.keras model eager_mode: False\n",
            "The ONNX operator number change on the optimization: 832 -> 223\n",
            "INFO:keras2onnx:The ONNX operator number change on the optimization: 832 -> 223\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****************************************************************************************************\n",
            "09:08:03\n",
            "MobileNetV2\n",
            "<function MobileNetV2 at 0x7ff40b977e18>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "tf executing eager_mode: True\n",
            "INFO:keras2onnx:tf executing eager_mode: True\n",
            "tf.keras model eager_mode: False\n",
            "INFO:keras2onnx:tf.keras model eager_mode: False\n",
            "The ONNX operator number change on the optimization: 437 -> 104\n",
            "INFO:keras2onnx:The ONNX operator number change on the optimization: 437 -> 104\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****************************************************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beuEuDiMPRAM",
        "colab_type": "text"
      },
      "source": [
        "# To Keras h5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeF_N4qAPUw6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "776bca10-da4b-498f-b8f1-3c2a50527c1e"
      },
      "source": [
        "print(f\"pram is {parm}\")\n",
        "if to_h5:\n",
        "  for name, func in zip(model_names, model_list):\n",
        "    now = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(now, name, func, sep = \"\\n\")\n",
        "    model = func()\n",
        "    target_path = os.path.join(target_folder, model.name+\".h5\")\n",
        "    model.save(target_path)\n",
        "    print(\"*\"*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pram is all\n",
            "09:08:10\n",
            "Xception\n",
            "<function Xception at 0x7ff40b994ae8>\n",
            "****************************************************************************************************\n",
            "09:08:14\n",
            "ResNet50\n",
            "<function ResNet50 at 0x7ff40b986ea0>\n",
            "****************************************************************************************************\n",
            "09:08:18\n",
            "InceptionV3\n",
            "<function InceptionV3 at 0x7ff40b977840>\n",
            "****************************************************************************************************\n",
            "09:08:25\n",
            "MobileNetV2\n",
            "<function MobileNetV2 at 0x7ff40b977e18>\n",
            "****************************************************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Nq2hY2Pc2z",
        "colab_type": "text"
      },
      "source": [
        "# go download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6clgAW-CLgk1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9396d15f-49d4-4102-c12a-f5663176cbb2"
      },
      "source": [
        "!tar zcvf converted_model.tar.gz target\n",
        "from google.colab import files\n",
        "files.download(\"converted_model.tar.gz\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target/\n",
            "target/inception_v3.h5\n",
            "target/model_ResNet50.tflite\n",
            "target/model_InceptionV3.tflite\n",
            "target/model_Xception_quantized.tflite\n",
            "target/model_MobileNetV2_quantized.tflite\n",
            "target/model_MobileNetV2.tflite\n",
            "target/ResNet50_1x224x224x3.onnx\n",
            "target/resnet50.h5\n",
            "target/model_Xception.tflite\n",
            "target/model_InceptionV3_quantized.tflite\n",
            "target/Xception_1x299x299x3.onnx\n",
            "target/mobilenetv2_1.00_224.h5\n",
            "target/xception.h5\n",
            "target/model_ResNet50_quantized.tflite\n",
            "target/MobileNetV2_1x224x224x3.onnx\n",
            "target/InceptionV3_1x299x299x3.onnx\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1d82a8a7-6fbb-4973-8c59-f7c2d078f120\", \"converted_model.tar.gz\", 681198856)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMyL1NA1Ouwh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "b24d6984-f85e-4820-b2b9-a2e8192b468c"
      },
      "source": [
        "!tar zcvf \"tpu_file.tar.gz\" *_edgetpu.*\n",
        "files.download(\"tpu_file.tar.gz\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_InceptionV3_quantized_edgetpu.log\n",
            "model_InceptionV3_quantized_edgetpu.tflite\n",
            "model_MobileNetV2_quantized_edgetpu.log\n",
            "model_MobileNetV2_quantized_edgetpu.tflite\n",
            "model_ResNet50_quantized_edgetpu.log\n",
            "model_ResNet50_quantized_edgetpu.tflite\n",
            "model_Xception_quantized_edgetpu.log\n",
            "model_Xception_quantized_edgetpu.tflite\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6234f9b2-29be-4860-b02c-b5bd7aa44246\", \"tpu_file.tar.gz\", 65545195)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUNLN5QqtOQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}