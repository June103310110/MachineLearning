{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from matplotlib.pylab import rcParams\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('featurefaultyA.csv')\n",
    "df['class'] = 1\n",
    "df_h = pd.read_csv('featureHealthyA.csv')\n",
    "df_h['class'] = 0\n",
    "\n",
    "data = pd.concat([df, df_h], axis = 0)\n",
    "y = data.pop('class')\n",
    "\n",
    "\n",
    "data_y=np_utils.to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set data dimension\n",
      "(12, 30)\n",
      "(12, 2)\n",
      "---------------------------------------\n",
      "training set: 12\n",
      "testing set: 8\n"
     ]
    }
   ],
   "source": [
    "# 從資料集切分為訓練集以及測試集，以便評估模型\n",
    "x_train, x_test, y_train, y_test \\\n",
    "    = train_test_split(data,\n",
    "                       data_y,\n",
    "                       test_size=0.4,)\n",
    "\n",
    "print('training set data dimension')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print('---------------------------------------')\n",
    "print('training set: {}'.format(len(x_train)))\n",
    "print('testing set: {}'.format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosFace(L.Layer):\n",
    "    def __init__(self, num_classes=10, scale=30.0, margin=0.35, **kwargs):\n",
    "        super(CosFace, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"W\",\n",
    "                                 shape=(input_shape[0][-1], self.num_classes),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # get embeddings and one hot labels from inputs\n",
    "        embeddings, onehot_labels = inputs\n",
    "        # normalize final W layer\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        # get logits from multiplying embeddings (batch_size, embedding_size) and W (embedding_size, num_classes)\n",
    "        logits = tf.matmul(embeddings, W)\n",
    "        # subtract margin from logits\n",
    "        target_logits = logits - self.margin\n",
    "        # get cross entropy\n",
    "        logits = logits * (1 - onehot_labels) + target_logits * onehot_labels\n",
    "        # apply scaling\n",
    "        logits = logits * self.scale\n",
    "        # get class probability distribution\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        return predictions\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_classes)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"scale\": self.scale,\n",
    "            \"margin\": self.margin})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 128)          3968        input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "BN_0 (BatchNormalization)       (None, 128)          512         dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           BN_0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 512)          66048       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "BN_vector (BatchNormalization)  (None, 512)          2048        dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 512)          0           BN_vector[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cos_face_6 (CosFace)            (None, 2)            1024        lambda_6[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 73,600\n",
      "Trainable params: 72,320\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "embedding_size = 512\n",
    "# pretrained_model = load_model(os.path.join(os.path.abspath(\n",
    "#     \"\"), \"models\", \"vggface2_train_softmax_\" + str(embedding_size) + \"d.h5\"))\n",
    "# pretrained_model = model\n",
    "\n",
    "\n",
    "onehot_labels = L.Input(shape=(num_classes,))\n",
    "# get feature vector extracted using DenseNet\n",
    "# feature_extractor = Model(inputs=pretrained_model.input,\n",
    "#                           outputs=model.get_layer(\"flatten\").output)\n",
    "\n",
    "\n",
    "inputs = L.Input(shape = (x_train.shape[1],))\n",
    "x = L.Dense(128, activation='tanh',\n",
    "#           input_shape=(x_train.shape[1],),\n",
    "            kernel_initializer=\"glorot_normal\",\n",
    "            name = 'dense_0'\n",
    "#           activity_regularizer=regularizers.l2(1e-5)\n",
    "           )(inputs)\n",
    "            \n",
    "# x = L.Flatten()(feature_extractor.output)\n",
    "# BN-Dropout-FC-BN\n",
    "x = L.BatchNormalization(name = 'BN_0')(x)\n",
    "x = L.Dropout(0.25)(x)\n",
    "x = L.Dense(embedding_size, activation=\"relu\")(x)\n",
    "x = L.BatchNormalization(name='BN_vector')(x)\n",
    "# make embeddings unit length\n",
    "embeddings = L.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(x)\n",
    "\n",
    "predictions = CosFace(num_classes=num_classes)([embeddings, onehot_labels])\n",
    "# model = Model(inputs=[feature_extractor.input,onehot_labels], \n",
    "#                       outputs=predictions)\n",
    "model = Model(inputs=[inputs,onehot_labels], \n",
    "                      outputs=predictions)\n",
    "\n",
    "model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\", \"<class 'pandas.core.frame.DataFrame'>\"}), <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # 每次看 batch_size 筆的資料就更新權重\n",
    "epochs = 15# 一個 epoch 會看過一次所有的資料\n",
    "\n",
    "\n",
    "# 訓練模型\n",
    "model_history = model.fit(x=[x_train, y_train], y=y_train, # 進行訓練 學習分布\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "#                           validation_data=(x_test, y_test),\n",
    "                          shuffle=True,\n",
    "                          verbose = False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 128)          3968        input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "BN_0 (BatchNormalization)       (None, 128)          512         dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           BN_0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 512)          66048       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "BN_vector (BatchNormalization)  (None, 512)          2048        dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 512)          0           BN_vector[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cos_face_6 (CosFace)            (None, 2)            1024        lambda_6[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 73,600\n",
      "Trainable params: 72,320\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'NoneType'>\", \"<class 'pandas.core.frame.DataFrame'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e417033d63fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                          )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    556\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "feature_extractor = Model(inputs=model.input,\n",
    "                          outputs=model.get_layer(\"BN_vector\").output)\n",
    "\n",
    "# inputs = L.Input(shape = (x_train.shape[1],))\n",
    "\n",
    "cls_layer = L.Dense(num_classes, activation='softmax')(feature_extractor.output)\n",
    "inf_model = Model(inputs = model.inputs, outputs = cls_layer)\n",
    "inf_model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "for layer in inf_model.layers:\n",
    "    layer.traninable = False\n",
    "for layer in inf_model.layers[-1:]:\n",
    "    layer.trainable = True\n",
    "model.summary()\n",
    "batch_size = 32  # 每次看 batch_size 筆的資料就更新權重\n",
    "epochs = 15 # 一個 epoch 會看過一次所有的資料\n",
    "\n",
    "\n",
    "# 訓練模型\n",
    "model_history = inf_model.fit(x=[x_train, None], y=y_train, # 進行訓練 學習分布\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          validation_data=(x_test, y_test),\n",
    "                          shuffle=True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Input has undefined rank:', TensorShape(None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f5f789924c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dense_0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BN_vector'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2139\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2141\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2142\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2143\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input has undefined rank:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0mndims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Input has undefined rank:', TensorShape(None))"
     ]
    }
   ],
   "source": [
    "inputs = L.Input(shape = (x_train.shape[1],))\n",
    "\n",
    "x = model.get_layer('dense_0')()\n",
    "x = L.BatchNormalization(name='BN_vector')(x)\n",
    "x = L.Dropout(0.25)(x)\n",
    "x = L.Dense(embedding_size, activation=\"relu\")(x)\n",
    "x = L.BatchNormalization(name='BN_vector')(x)\n",
    "\n",
    "cls_layer = L.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "inf_model = Model(inputs = inputs, outputs = cls_layer)\n",
    "inf_model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# 進行推論\n",
    "num_classes = 2\n",
    "\n",
    "for layer in inf_model.layers:\n",
    "    layer.traninable = False\n",
    "for layer in inf_model.layers[-1:]:\n",
    "    layer.trainable = True\n",
    "model.summary()\n",
    "batch_size = 32  # 每次看 batch_size 筆的資料就更新權重\n",
    "epochs = 15 # 一個 epoch 會看過一次所有的資料\n",
    "\n",
    "\n",
    "# 訓練模型\n",
    "model_history = inf_model.fit(x=x_train, y=y_train, # 進行訓練 學習分布\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          validation_data=(x_test, y_test),\n",
    "                          shuffle=True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4825076  0.5174924 ]\n",
      " [0.47830367 0.5216963 ]\n",
      " [0.4804379  0.51956207]\n",
      " [0.47366205 0.526338  ]\n",
      " [0.48239055 0.5176094 ]\n",
      " [0.48256227 0.5174377 ]\n",
      " [0.48030272 0.5196973 ]\n",
      " [0.48823005 0.5117699 ]\n",
      " [0.47971672 0.5202832 ]\n",
      " [0.52339005 0.47660992]\n",
      " [0.4825929  0.5174071 ]\n",
      " [0.48629394 0.513706  ]]\n"
     ]
    }
   ],
   "source": [
    "a = inf_model.predict([x_train, np.array([[0, 0]])])\n",
    "print(a)\n",
    "y_pred = []\n",
    "for i in a:\n",
    "    y_pred.append(np.argmax(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for i in y_train:\n",
    "    y_true.append(np.argmax(i))\n",
    "#     print(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.09090909]),\n",
       " array([0. , 0.5]),\n",
       " array([0.        , 0.15384615]),\n",
       " array([10,  2]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# 这部分返回一个张量\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "# 层的实例是可调用的，它以张量为参数，并且返回一个张量\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# 这部分创建了一个包含输入层和三个全连接层的模型\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
